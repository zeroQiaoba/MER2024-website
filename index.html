<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="MER24@IJCAI and MRAC24@ACM MM">
    <meta property="og:image" content="./img/mer2023.jpg">
    <meta property="og:type" content="website">
    
    <title>
        MER24@IJCAI and MRAC24@ACM MM
    </title>

    <!-- Bootstrap Core CSS -->
    <link href="./css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="./css/agency.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="./css/font-awesome.min.css" rel="stylesheet" >
    <link href="./css/style.css" rel="stylesheet">
    <!-- <link href="./css/css(1)" rel="stylesheet" type="text/css">
    <link href="./css/css(2)" rel="stylesheet" type="text/css">
    <link href="./css/css(3)" rel="stylesheet" type="text/css"> -->

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script type="text/javascript" async="" src="./js/analytics.js"></script>
    <script async="" src="./js/js"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'UA-114495477-1');
    </script>


</head>

<body id="page-top" class="index">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="hidden active">
                        <a href="https://MER2024.github.io/"></a>
                    </li>
                    <li class="">
                        <a class="page-scroll" href="#introduction">Intro</a>
                    </li>
					<li>
                        <a class="page-scroll" href="#news">News</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#challenge">Challenge</a>
                    </li>
					<li>
                        <a class="page-scroll" href="#workshop">Workshop</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#schedule">Schedule</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#speakers">Speakers</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#organization">Organizers</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#program_committee">Program Committee</a>
                    </li>

                   
                </ul>
            </div>
        </div>
    </nav> 

    <!-- Header -->
    <header_3>
        <div class="container">
            <div class="intro-text" style="color:white;">
                <br>        
                <div class="intro-heading">MER24@IJCAI and MRAC24@ACM MM</div>
                <br><br>
                <br><br>                
                <div class="intro-heading-large">Multimodal Emotion</div>
                <br><br> 
                <div class="intro-heading-large">Recognition</div>
                <br><br> 
                <br><br>              
                <div class="intro-heading"> Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Emotion Recognition</div>
                <br><br>
            </div>
        </div>
    </header_3>

    <!-- Introduction Section -->
    <section id="introduction" style="padding-top: 30px;">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                </div>
            </div>
            <div class="row text-justify">
                <div class="col-md-12" >

                    <p class="large text-muted ">
                
                        <br>Multimodal emotion recognition is an active research topic in artificial intelligence. 
                        Its main goal is to integrate multi-modalities to identify human emotional states. Current works generally assume accurate emotion labels for benchmark datasets and focus on developing more effective architectures.
                        However, existing technologies are difficult to meet the demand for practical applications. 
                        To this end, last year, we launched <a href="http://merchallenge.cn/mer2023">MER23@ACM Multimedia</a> and <a href="http://merchallenge.cn/workshop">MRAC23@ACM Multimedia</a>.
                        In this year, we will continuously holding related workshops and challenges that bring together researchers around the world to further discuss recent research and future directions for robust multimodal emotion recognition.</br>
                        
                        <br>In this workshop and challenge, we aim to bring together researchers from the fields of multimodal modeling of human affect, 
                        modality robustness of affect recognition, low-resource affect recognition, human affect synthesis in multimedia, 
                        privacy in affective computing, and applications in health, education, entertainment, etc., 
                        to further discuss recent research and future directions for affective computing in multimedia. 
                        At the same time, we intend to provide a communication platform for all participants of MER24@IJCAI, 
                        to systematically evaluate the robustness of emotion recognition systems and promote applications of this technology in practice.</br>
                    </p>

                </div>
            </div>
        </div>
    </section>


<!-- Call for Papers Section -->
<section id="news" class="bg-mid-gray">
    <div class="container">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">News</h2>
                <!-- <h3 class="section-subheading text-muted" style="font-size:large">Contact with us: <a href="mailto:fl4pwsdm@gmail.com">fl4pwsdm@gmail.com</a></h3> -->
            </div>
        </div>
        <div class="text-left" style="text-align: center;">
            <div class="col-md-12">
                <p class="large text-muted">
                <span> April 30, 2024: </span><b>We establish an initial website for MER24@IJCAI and MRAC24@ACM MM</b>
                </p>
            </div>
            
            <div class="col-md-12">
                <p class="large text-muted">
                <span> May 7, 2024: </span><b>For MER-OV, we require that participants cannot use closed-source models (such as GPT or Claude). </b>
                </p>
            </div>
            <div class="col-md-12">
                <p class="large text-muted">
                <span> May 26, 2024: </span><b>For MER-OV, we update the <a href="https://github.com/zeroQiaoba/MERTools/tree/master/MER2024">evaluation code </a>, <a href="https://arxiv.org/abs/2404.17113"> baseline paper </a> and <a href="https://arxiv.org/pdf/2306.15401"> EMER</a>.</b>
                </p>
            </div>
            <div class="col-md-12">
                <p class="large text-muted">
                <span> June 18, 2024: </span><b>For all tracks, the final submitted labels should be in English.</b>
                </p>
            </div>
            <div class="col-md-12">
                <p class="large text-muted">
                <span> June 18, 2024: </span><b>For MER-OV, final-openset-*.csv in the provided dataset is the labels extracted from EMER descriptions. <a href="https://github.com/zeroQiaoba/MERTools/blob/master/MER2024/ov_store/check-openset.csv"> check-openset.csv </a> in github is the final checked ground truths. </b>
                </p>
            </div>
            <div class="col-md-12">
                <p class="large text-muted">
                <span> June 21, 2024: </span><b>We provide CodaLab links for three tracks. To reduce the difficulty, we limit the evaluation scope to <a href="https://drive.google.com/file/d/14agT9WeyFtzBtym-WAXbda1qq1PD6KDI/view?usp=sharing"> 20,000 samples </a> and provide pre-extracted audio and subtitles for unlabeled data in <a href="https://terabox.com/s/1t_MUZTttRuN7ZKhxx_44Ww"> [link]. </a> </b>
                </p>
            </div>
            <div class="col-md-12">
                <p class="large text-muted">
                <span> July 10, 2024: </span><b>The competition results submission deadline has now passed. We would like to express our sincere gratitude for your support during MER24. We encourage all participants to submit papers to MRAC24@ACM Multimedia. </b>
                </p>
            </div>

            <div class="col-md-12">
                <p class="large text-muted">
                <span> Sep 17, 2024: </span><b> All accepted papers are listed on this website. </b>
                </p>
            </div>

            <div class="col-md-12">
                <p class="large text-muted">
                <span> Oct 10, 2024: </span><b> We release the workshop schedule. Hope to see you in Melbourne. </b>
                </p>
            </div>

        </div>
    </div>
</section>	
	
	
<!-- Call for Papers Section -->
<section id="challenge">
    <div class="container">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">MER24 Challenge@IJCAI</h2>
                <!-- <h3 class="section-subheading text-muted" style="font-size:large">Contact with us: <a href="mailto:fl4pwsdm@gmail.com">fl4pwsdm@gmail.com</a></h3> -->
            </div>
        </div>
        <div class="row text-justify">
            <div class="col-md-12">
                <p class="large text-muted">
                    Compared with previous MER23, in this year, we enlarge the dataset size by including more labeled and unlabeled samples. 
                    Meanwhile, besides MER-SEMI and MER-NOISE, we introduce a new track called MER-OV.
                </p>

                <br>
				<p class="large text-muted">
                    <b>Track 1. MER-SEMI.</b> It is difficult to collect large amounts of samples with emotion labels. 
                    To address the problem of data sparseness, researchers revolve around unsupervised or semi-supervised learning and use unlabeled data during training. 
                    Furthermore, MERBench [1] points out the necessity of using unlabeled data from the same domain as labeled data. 
                    Therefore, we provide a large number of human-centric unlabeled videos in MER2024 and encourage participants 
                    to explore more effective unsupervised or semi-supervised learning strategies for better performance.
                </p>
				
                <p class="large text-muted">
                    <b>Track 2. MER-Noise.</b> Noise generally exists in the video. 
                    It is hard to guarantee that every video is free of any audio noise and each frame is in high-resolution. 
                    To improve noise robustness, researchers have carried out a series of works on emotion recognition under noise conditions. 
                    However, there lacks a benchmark dataset to fairly compare different strategies. 
                    Therefore, we organize a track around noise robustness. 
                    Although there are many types of noise, we consider the two most common ones: audio additive noise and image blur noise. 
                    We encourage participants to exploit data augmentation [2] or other techniques [3] to improve the noise robustness of emotion recognition systems.
                </p>

                <p class="large text-muted">
                    <b>Track 3. MER-OV.</b> Emotions are subjective and ambiguous. To increase annotation consistency, 
                    existing datasets typically limit the label space to a few discrete labels, employ multiple annotators, and use majority voting to select the most likely label. 
                    However, this process may cause some correct but non-candidate or non-majority labels to be ignored, resulting in inaccurate annotations. 
                    To this end, we introduce a new track on open-vocabulary emotion recognition. 
                    We encourage participants to generate any number of labels in any category, trying to describe the emotional state accurately [4].
                </p>

                <br>
                <head>
                    <title>MathJax Example</title>
                    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
                    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
                </head>
                <p class="large text-muted">
                    <b>Evaluation Metrics.</b> For MER-SEMI and MER-NOISE, 
                    we choose two widely used metrics in emotion recognition: accuracy and weighted average F-score (WAF). 
                    Considering the inherent class imbalance, we choose WAF as the final ranking.
                    For MER-OV, we draw on our previous work [4], in which we extend traditional classification metrics (i.e., accuracy and recall) and define set-level accuracy and recall.
                    More details can be found in our baseline code. 
    
                </p>
                
                <br>
                <p class="large text-muted">
                    <b>Dataset.</b> Please download the <a href="https://drive.google.com/file/d/1cXNfKHyJzVXg_7kWSf_nVKtsxIZVa517/view?usp=sharing">End User License Agreement</a>, 
                    fill it out and send it to <a href="merchallenge.contact@gmail.com">merchallenge.contact@gmail.com</a> to access the data. 
                    We will review your application and get in touch as soon as possible. 
                    EULA requires participants to use this dataset only for academic research and not to edit or upload samples to the Internet. 
                    
                </p>
                
                <br>
                <p class="large text-muted">
                    <b>Result submission.</b> For MER-SEMI and MER-NOISE, each team should submit the most likely discrete label among the 6 candidate labels 
                    (i.e., <i> worried, happy, neutral, angry, surprise, and sad </i>). 
                    For MER-OV, each team can submit any number of labels in any category. 
                    <b>For three tracks, participants should predict results of <a href="https://drive.google.com/file/d/14agT9WeyFtzBtym-WAXbda1qq1PD6KDI/view?usp=sharing"> 20,000 samples </a> from 115,595 unlabeled data</b>, although we only evaluate a small subset. 
                    To focus on generalization performance rather than optimizing for a specific subset, 
                    we do not provide information about which samples belong to the test subset.
                    <b>For MER-OV, participants can  only use open-source LLMs, MLLMs, or other models. Closed source models (such as GPT Series or Claude Series models) are not allowed.</b>
                </p>
                
                <p style="color: rgb(135,206,235)">CodaLab link for MER2024-SEMI:  <a href="https://codalab.lisn.upsaclay.fr/competitions/19437"> https://codalab.lisn.upsaclay.fr/competitions/19437  </a> </p>
                <p style="color: rgb(135,206,235)">CodaLab link for MER2024-NOISE: <a href="https://codalab.lisn.upsaclay.fr/competitions/19438"> https://codalab.lisn.upsaclay.fr/competitions/19438  </a> </p>
                <p style="color: rgb(135,206,235)">CodaLab link for MER2024-OV:    <a href="https://codalab.lisn.upsaclay.fr/competitions/19439"> https://codalab.lisn.upsaclay.fr/competitions/19439  </a> </p>
                
                <p class="large text-muted">
                    <b> Note: </b> please register on Codalab using the email provided on the EULA or the email where you sent the EULA. 
                    The rankings of MER2024-SEMI and MER2024-NOISE are based on the CodaLab leaderboard. 
                    But for MER2024-OV, CodaLab is only used for format check. 
                    Each team can submit five times to the official email, the best performance among them used for final ranking​​.
                    Due to slightly randomness of GPT-3.5, we will run the evaluation code five times and report the average score. 
                    The ranking of MER2024-OV track will be announced a few weeks after the competition ends.
                </p>

                <br>
                <p class="large text-muted">
                    <b>Paper submission.</b> All participants are encouraged to submit a paper describing their solution to the MRAC24 Workshop@ACM Multimedia.
                    Top-5 teams in each track <b>MUST</b> submit a paper.
                    Top-3 winning teams in each track will be awarded with a certificate.
                    Paper sumbission link: <a href="https://cmt3.research.microsoft.com/MRAC2024"> https://cmt3.research.microsoft.com/MRAC2024 </a>
                </p>
                
                <br>
                <p class="large text-muted">
                    <b>Baseline paper:</b> <a href="https://arxiv.org/abs/2404.17113"> https://arxiv.org/abs/2404.17113 </a>
                    <br>
                    <b>Baseline code:</b> <a href="https://github.com/zeroQiaoba/MERTools/tree/master/MER2024">https://github.com/zeroQiaoba/MERTools/tree/master/MER2024 </a>
                    <br>
                    <b>Contact email:</b> <a href="merchallenge.contact@gmail.com">merchallenge.contact@gmail.com</a>
                </p>

                <br>

                <br>
                <p class="text-muted"> 
					[1] Zheng Lian, Licai Sun, Yong Ren, Hao
                    Gu, Haiyang Sun, Lan Chen, Bin Liu, and Jianhua
                    Tao. 
                    <a href="https://arxiv.org/pdf/2401.03429.pdf">Merbench: A unified evaluation benchmark for multimodal emotion recognition. </a>
                    arXiv preprint
                    arXiv:2401.03429, 2024.
					<br>
                    [2] Devamanyu Hazarika, Yingting Li,
                    Bo Cheng, Shuai Zhao, Roger Zimmermann, and Soujanya Poria. 
                    <a href="https://aclanthology.org/2022.naacl-main.50.pdf">Analyzing modality robustness in multimodal sentiment analysis.</a>
                     In Proceedings of the North American
                    Chapter of the Association for Computational Linguistics:
                    Human Language Technologies, pages 685–696, 2022.
                    <br>
                    [3] Zheng Lian, Lan Chen, Licai Sun, Bin
                    Liu, and Jianhua Tao. 
                    <a href="https://ieeexplore.ieee.org/abstract/document/10008078">Gcnet: Graph completion network for incomplete multimodal learning in conversation.</a>
                    IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(07):8419–8432, 2023.
                    
                    <br>
                    [4] Zheng Lian, Haiyang Sun, Licai Sun, Hao Gu, Zhuofan Wen, Siyuan Zhang, Shun Chen, Mingyu Xu, Ke Xu, Kang Chen, Lan Chen, Shan Liang, Ya Li, Jiangyan Yi, Bin Liu, and Jianhua Tao. 
                    <a href="https://arxiv.org/abs/2306.15401">Explainable Multimodal Emotion Recognition.</a>
                    arXiv preprint arXiv:2306.15401, 2023.

                    <br>
                    [5] Zheng Lian, Haiyang Sun, Licai Sun, Zhuofan Wen, Siyuan Zhang, Shun Chen, Hao Gu, Jinming Zhao, Ziyang Ma, Xie Chen, Jiangyan Yi, Rui Liu, Kele Xu, Bin Liu, Erik Cambria, Guoying Zhao, Björn W. Schuller, and Jianhua Tao. 
                    <a href="https://arxiv.org/abs/2404.17113">MER 2024: Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition.</a>
                    arXiv preprint arXiv:2404.17113, 2024.
                </p>
            </div>
        </div>
    </div>
</section>


<section id="workshop" class="bg-mid-gray">
    <div class="container">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">MRAC24 Workshop@ACM MM</h2>
                <!-- <h3 class="section-subheading text-muted" style="font-size:large">Contact with us: <a href="mailto:fl4pwsdm@gmail.com">fl4pwsdm@gmail.com</a></h3> -->
            </div>
        </div>
        <div class="row text-justify">
            <div class="col-md-12" >

                <p class="large text-muted ">
                    <br><br>Besides papers for the MER24 Challenge, we also invite submissions on any aspect of multimodal emotion recognition and synthesis in deep learning. 
                    <b>Topics include but not limited to:</b> </br>
                </p>


                <ul class="large text-muted">
                    
                    <td style="width: 45%; vertical-align:top"><li> Multimodal modeling of human affect</li></td>
                    <td style="width: 45%; vertical-align:top"><li> Modality robustness of affect recognition</li></td>
                    <td style="width: 45%; vertical-align:top"><li> LLM-based and MLLM-based emotion recognition</li></td>
                    <td style="width: 45%; vertical-align:top"><li> Open-set emotion recognition</li></td>
                    <td style="width: 45%; vertical-align:top"><li> Low-resource affect recognition</li></td>
                    <td style="width: 45%; vertical-align:top"><li> Contextualized modeling of affective states</li></td>
                    <td style="width: 45%; vertical-align:top"><li> Human affect synthesis in multimedia</li></td>
                    <td style="width: 45%; vertical-align:top"><li> Privacy in affective computing</li></td>
                    <td style="width: 45%; vertical-align:top"><li> Explainable, fair, trustworthy in affective computing</li></td>
                    <td style="width: 45%; vertical-align:top"><li> Applications in health, education, entertainment, etc.</li></td>
                    
                </ul>

                <p class="large text-muted ">
                    <br>
                    <b>Format:</b> Submitted papers (.pdf format) must use the ACM Article Template: <a href="https://2024.acmmm.org/files/ACM-MM24-paper-templates.zip">paper template</a>. 
                    Please use the template in traditional double-column format to prepare your submissions. 
                    For example, word users may use <b>Word Interim Template</b>, and latex users may use <b>sample-sigconf-authordraft</b> template. 
                    When using sample-sigconf-authordraft template, please comment all the author information for submission and review of manuscript, 
                    instead of changing the documentclass command to <b>'\documentclass[manuscript, screen, review]{acmart}'</b> as told by instructions. 
                    Please ensure that you submit your papers subscribing to this format for full consideration during the review process.

                    <br>
                    <br>

                    <b>Length:</b> The manuscript’s length is limited to one of the two options: 
                    <b>a) 4 pages plus 1-page reference; or b) 8 pages plus up to 2-page reference.</b>
                    The reference pages must only contain references. Overlength papers will be rejected without review.
                    Papers should be single-blind.
                    We do not allow appendix that follow right after the main paper in the main submission file.

                    <br>
                    <br>

                    <b>Peer Review and publication in ACM Digital Library:</b> 
                    Paper submissions must conform with the “double-blind” review policy. 
                    All papers will be peer-reviewed by experts in the field, they will receive at least two reviews. 
                    Acceptance will be based on relevance to the workshop, scientific novelty, and technical quality.
                    The workshop papers will be published in the ACM Digital Library.

                    <br>
                    <br>

                    <b>Accepted Papers:</b> 
                    
                    <br>
                    <b> [Baseline] <a href="https://arxiv.org/abs/2404.17113">MER 2024: Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition.</a></b>
                    Zheng Lian, Haiyang Sun, Licai Sun, Zhuofan Wen, Siyuan Zhang, Shun Chen, Hao Gu, Jinming Zhao, Ziyang Ma, Xie Chen, Jiangyan Yi, Rui Liu, Kele Xu, Bin Liu, Erik Cambria, Guoying Zhao, Björn W. Schuller, Jianhua Tao
                    <br>
                    <br>

                    <b> [MER-SEMI] <a href="https://arxiv.org/pdf/2409.07078">Multimodal Emotion Recognition with Vision-language Prompting and Modality Dropout.</a></b>
                    Anbin Qi, Zhongliang Liu, Xinyong Zhou, Jinba Xiao, Fengrun Zhang, Qi Gan, Ming Tao, Gaozheng Zhang, Lu Zhang
                    <br>
                    <b> [MER-SEMI] <a href="https://arxiv.org/pdf/2409.18971">Early Joint Learning of Emotion Information Makes MultiModal Model Understand You Better.</a></b>
                    Mengying Ge, Mingyang Li, Dongkai Tang, Pengbo Li, Kuo Liu, Shuhao Deng, Songbai Pu, Long Liu, Yang Song, Tao Zhang
                    <br>
                    <b> [MER-SEMI] <a href="https://arxiv.org/pdf/2409.05007">Audio-Guided Fusion Techniques for Multimodal Emotion Analysis.</a></b>
                    Pujin Shi, Fei Gao
                    <br>
                    <b> [MER-SEMI] <a href="https://arxiv.org/pdf/2409.05015">Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment.</a></b>
                    Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, Lei Xie
                    <br>
                    <b> [MER-SEMI] <a href="https://arxiv.org/pdf/2409.04447">Leveraging Contrastive Learning and Self-Training for Multimodal Emotion Recognition with Limited Labeled Samples.</a></b>
                    Qi Fan, Yutong Li, Yi Xin, Xinyu Cheng, Guanglai Gao, Miao Ma
                    <br>
                    <br>

                    <b> [MER-NOISE] <a href="https://arxiv.org/pdf/2408.10500">SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition.</a>  </b>
                    Zebang Cheng, Shuyuan Tu, Dawei Huang, Minghan Li, Xiaojiang Peng, Zhi-Qi Cheng, Alexander G. Hauptmann
                    <br>
                    <b> [MER-NOISE] <a href="https://arxiv.org/pdf/2409.18971">Early Joint Learning of Emotion Information Makes MultiModal Model Understand You Better.</a></b>
                    Mengying Ge, Mingyang Li, Dongkai Tang, Pengbo Li, Kuo Liu, Shuhao Deng, Songbai Pu, Long Liu, Yang Song, Tao Zhang
                    <br>
                    <b> [MER-NOISE] <a href="xxx">Multimodal Blockwise Transformer for Robust Sentiment Recognition.</a></b>
                    Zhengqin Lai, Xiaopeng Hong, Yabin Wang
                    <br>
                    <b> [MER-NOISE] <a href="xxx">Robust Representation Learning for Multimodal Emotion Recognition with Contrastive Learning and Mixup.</a></b>
                    Yunrui Cai, Runchuan Ye, Jingran Xie, Yaoxun Xu, Yixuan Zhou, Zhiyong Wu
                    <br>
                    <br>

                    <b> [MER-OV] <a href="https://arxiv.org/pdf/2408.11286">Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model.</a></b>
                    Mengying Ge, Dongkai Tang, Mingyang Li
                    <br>
                    <b> [MER-OV] <a href="xxx">Open Vocabulary Emotion Prediction Based on Large Multimodal Models.</a></b>
                    Zixing Zhang, Zhongren Dong, Zhiqiang Gao, Shihao Gao, Donghao Wang, Ciqiang Chen, Yuhan Nie, Huan Zhao
                    <br>
                    <b> [MER-OV] <a href="https://arxiv.org/pdf/2408.10500">SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition.</a>  </b>
                    Zebang Cheng, Shuyuan Tu, Dawei Huang, Minghan Li, Xiaojiang Peng, Zhi-Qi Cheng, Alexander G. Hauptmann
                    <br>
                    <b> [MER-OV] <a href="xxx">Multimodal Emotion Captioning Using Large Language Model with Prompt Engineering.</a></b>
                    Yaoxun Xu, Yixuan Zhou, Yunrui Cai, Jingran Xie, Runchuan Ye, Zhiyong Wu
                    <br>
                    <br>

                    <b> <a href="https://arxiv.org/abs/2407.16552">MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Subtle Clue Dynamics in Video Dialogues.</a></b>
                    Liyun Zhang, Zhaojie Luo, Shuqiong Wu, Yuta Nakashima
                    <br>
                    <b> <a href="https://arxiv.org/pdf/2311.16114">Learning Noise-Robust Joint Representation for Multimodal Emotion Recognition under Incomplete Data Scenarios.</a></b>
                    Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian, Guanglai Gao
                    <br>
                    <br>
                </p>

                <p class="large text-muted">
                    <b>Contact email:</b>  merchallenge.contact@gmail.com
                </p>


            </div>
        </div>
    </div>
</section>



 <!--Key Dates Section -->
 <section id="schedule">
    <div class="container">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">Schedule</h2>
                <!-- <h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3--> 
            </div>
        </div>
        <div class="row">
            <div class="col-md-12 text-left">
                &nbsp;
            </div>
            <div class="text-left" style="text-align: center;">
                <div class="col-md-12">

                    <p class="large text-muted">
                        <del><span>April 30, 2024:</span> <b>Data, baseline paper & code available</b> </del>
                    </p>
                    
                    <p class="large text-muted">
                        <del><span> June 26, 2024:</span> <b>Results submission start</b> </del>
                    </p>

                    <p class="large text-muted">
                        <del><span> July 10, 2024:</span> <b>Results submission deadline</b></del>
                    </p>

                    <p class="large text-muted">
                        <del><span> July 20, 2024:</span> <b>Paper submission deadline</b></del>
                    </p>

                    <p class="large text-muted">
                        <del><span> August 5, 2024:</span> <b>Paper acceptance notification</b></del>
                    </p>

                    <p class="large text-muted">
                        <del><span> August 18, 2024:</span> <b>Deadline for camera-ready papers</b></del>
                    </p>

                    <p class="large text-muted">
                        <span> November 1 PM, 2024:</span> <b>MRAC24 workshop@ACM MM</b>
                    </p>

                    <br>

                    <p class="large text-muted">
                        <span> All submission deadlines are at 23:59 Anywhere on Earth (AoE).</b>
                    </p>
                    
                </div>
                <!-- <iframe src="img/schedule.pdf" width="600" height="500"></iframe> -->
                <!-- <embed src="img/schedule.pdf" width="1200" height="1000" type="application/pdf"> -->
                
                <div class="row text-justify">
                    <div class="col-md-12" >
        
                        <p class="large text-muted ">
                            <br>
                            <b>Each speaker is required to attend in person, and each paper will be presented orally for a total of 10 minutes (8 minutes for the presentation and 2 minutes for questions).</b>
                        </p>
                    </div>
                </div>
                <img src="img/schedule.png" alt="schedule" width="1160" height="800">
            </div>
        </div>
    </div>
</section> 



    <!-- Speakers Section -->

    <section id="Speakers"  class="bg-mid-gray" >
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Speakers</h2>
                </div>
            </div>
         
            
            <div class="row speakers">
                <div class="col-sm-4">
                    <div class="team-member">
                        <a href="https://zitongyu.github.io" target="_blank">
                            <img src="./img/zitong.png" style="height: 200px; width:200px;" class="img-responsive img-circle">
                        </a>
                        <h4><a href="https://zitongyu.github.io/" target="_blank">Zitong Yu</a></h4>
                        <p class="text-muted">Assistant Professor <br> Great Bay University </p>

                    </div>
                </div>

                <div class="col-sm-8 text-justify ">
                    <h3>Title: Facial Physiological and Emotional Analysis.
                    </h3>
                    <p class="text-muted" style="font-size: 16px;">
                        Besides biometric info, there are rich physiological and emotional clues from human faces. 
                        Thanks to the rapid development of the AI/CV community, instead of handcrafted expert-level features, many vision foundation models 
                        and efficient learning methods are designed for subtle facial physiological and emotional analysis. This talk will explore the 
                        technological advancements in facial video-based remote photoplethysmography (rPPG), facial action unit (AU) detection, 
                        and emotion recognition systems. Furthermore, a downstream real-world application on deception detection will be introduced. 
                        Finally, some challenges and future directions will be discussed. 
                    </p>
                </div>
            </div>


    </section>






    <!-- Organization Section -->
    <section id="organization">

            <div class="row">
                <div class="col-lg-12 text-center" style="padding-bottom: 20px;">
                    <h2 class="section-heading">ORGANISERS</h2>
                </div>
            </div>

            <div class="container">
                <div class="row">
                    <div class="col-sm-1">
                        &nbsp;
                    </div>

                    
                    <div class="col-sm-2">
                        <div class="team-member">
                            <img src="./img/jhtao.png" class="img-responsive img-circle" alt="" width="200" height="300">
                            <h4><a href="https://www.au.tsinghua.edu.cn/info/1080/3219.htm">Jianhua Tao</a></h4>
                            <p class="text-muted"> Tsinghua University </p>
                        </div>
                    </div>

                    

                    <div class="col-sm-2">
                        &nbsp;
                    </div>
					
                    <div class="col-sm-2">
                        <div class="team-member">
                            <img src="./img/lianzheng.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                            <h4><a href="https://zeroqiaoba.github.io/Homepage/">Zheng Lian</a></h4>
                            <p class="text-muted"> Institute of Automation, Chinese Academy of Sciences</p>
                        </div>
                    </div>

					
					
                    

                    <div class="col-sm-2">
                        &nbsp;
                    </div>

                    <div class="col-sm-2">
                        <div class="team-member">
                            <img src="./img/Björn W. Schuller.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                            <h4><a href="http://www.schuller.one/">Björn W. Schuller</a></h4>
                            <p class="text-muted">  Imperial College London </p>
                        </div>
                    </div>

                  </div>

              </div>

                
            <div class="container">
              <div class="row">
                <div class="col-sm-1">
                    &nbsp;
                </div>
                <div class="col-sm-2">
                    &nbsp;
                </div>
               

                <div class="col-sm-2">
                  <div class="team-member">
                      <img src="./img/Guoying_2023.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                      <h4><a href="https://gyzhao-nm.github.io/Guoying/"> Guoying Zhao </a></h4>
                      <p class="text-muted">  University of Oulu </p>
                  </div>
                </div>

                <div class="col-sm-2">
                    &nbsp;
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/Erik Cambria.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://sentic.net/team/"> Erik Cambria </a></h4>
                        <p class="text-muted"> Nanyang Technological University </p>
                    </div>
                </div>

                <div class="col-sm-2">
                    &nbsp;
                </div>

              </div>
            </div>

        </div>
		
		    
        <div class="row" >
            <div class="col-lg-12 text-center" style="padding-bottom: 20px;">
                <h2 class="section-heading">Data Chairs</h2>
            </div>
        </div>

        <div class="container">
            <div class="row">
                <div class="col-sm-1">
                    &nbsp;
                </div>

                
				<div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/liubin.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://people.ucas.edu.cn/~bin.liu">Bin Liu</a></h4>
                        <p class="text-muted"> Institute of Automation, Chinese Academy of Sciences </p>
                    </div>
                </div>
                
				
				<div class="col-sm-2">
                    &nbsp;
                </div>
				
				<div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/liurui.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://ttslr.github.io/people.html">Rui Liu</a></h4>
                        <p class="text-muted"> Inner Mongolia University </p>
                    </div>
                </div>
				
				
				
				<div class="col-sm-2">
                    &nbsp;
                </div>
				
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/xukele.png" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://kelelexu.github.io/">Kele Xu</a></h4>
                        <p class="text-muted"> National University of Defense Technology</p>
                    </div>
                </div>

              </div>

          </div>
        </div>
    </section>


    <section id="program_committee" class="bg-mid-gray">

        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Program Committee</h2>
                </div>
            </div>

			<br>
            <br>

            <div class="row">
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/xiaobaili2.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://scholar.google.com/citations?user=JTFfexYAAAAJ&hl=en&oi=ao">Xiaobai Li</a></h4>
                        <p class="text-muted">Zhejiang University</p>
                    </div>
                </div>

                <div class="col-sm-1">
                    &nbsp;
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/zixingzhang.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://scholar.google.com/citations?user=S998M7cAAAAJ&hl=en&oi=ao">Zixing Zhang</a></h4>
                        <p class="text-muted"> Hunan University </p>
                    </div>
                </div>

                <div class="col-sm-1">
                    &nbsp;
                </div>

				<div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/jianfeiyu.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://sites.google.com/site/jfyu1990/">Jianfei Yu</a></h4>
                        <p class="text-muted"> Nanjing University of Science and Technology </p>
                    </div>
                </div>

                <div class="col-sm-1">
                    &nbsp;
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/liya.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://scholar.google.com/citations?user=ISK42qAAAAAJ&hl=en&oi=ao">Ya Li</a></h4>
                        <p class="text-muted"> Beijing University of Posts and Telecommunications </p>
                    </div>
                </div>


            </div>

            <div class="row">
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/mengyuewy.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://myw19.github.io/">Mengyue Wu</a></h4>
                        <p class="text-muted"> Shanghai Jiao Tong University </p>
                    </div>
                </div>
                
                <div class="col-sm-1">
                    &nbsp;
                </div>
                
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/hanjin.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://sites.google.com/view/jinghan">Jing Han</a></h4>
                        <p class="text-muted">University of Cambridge</p>
                    </div>
                </div>

                <div class="col-sm-1">
                    &nbsp;
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/jinming.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://scholar.google.com/citations?user=XskCnD8AAAAJ&hl=en&oi=ao">Jinming Zhao</a></h4>
                        <p class="text-muted"> Qiyuan Lab </p>
                    </div>
                </div>
                
                <div class="col-sm-1">
                    &nbsp;
                </div>
                
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/mingyue.gif" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://scholar.google.com/citations?user=lrr05v8AAAAJ&hl=en&oi=ao">Mingyue Niu</a></h4>
                        <p class="text-muted"> Yanshan University </p>
                    </div>
                </div>
                    
            </div>

            
            <div class="row">
                
                <div class="col-sm-2">
                    &nbsp;
                </div>
                
                <div class="col-sm-1">
                    &nbsp;
                </div>
                
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/yongwei.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="xxx">Yongwei Li</a></h4>
                        <p class="text-muted">Institute of Psychology, Chinese Academy of Sciences</p>
                    </div>
                </div>

                <div class="col-sm-1">
                    &nbsp;
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/lica.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://sunlicai.github.io/">Licai Sun</a></h4>
                        <p class="text-muted"> University of Oulu </p>
                    </div>
                </div>
                    
            </div>


            

          </div>
        </div>

    </section>



    <!-- jQuery -->
    <script src="./js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="./js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="./js/jquery.easing.min.js"></script>
    <script src="./js/classie.js"></script>
    <script src="./js/cbpAnimatedHeader.js"></script>

    <!-- Contact Form JavaScript -->
    <script src="./js/contact_me.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="./js/agency.js"></script>




</body></html>
